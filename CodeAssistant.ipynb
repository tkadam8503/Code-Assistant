{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5ad785-cfee-4103-abb6-184ff59ff4b9",
   "metadata": {},
   "source": [
    "# Code Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec902a-45ef-4f1f-b843-fd973029b92c",
   "metadata": {},
   "source": [
    "## What’s inside this notebook\n",
    "\n",
    "This notebook is a compact, teachable example of a *code-generating agent* you can run locally:\n",
    "\n",
    "- **Generator** — turns a natural-language task into Python code  \n",
    "- **Runner** — executes the code in a temp sandbox and captures outputs/errors  \n",
    "- **Auto-debugger** — if the first run fails, asks the model (or a rule) to fix the code once  \n",
    "- **Evaluator** — checks simple tasks against assertions (offline)  \n",
    "\n",
    "> Why local-first?  \n",
    "> - **Zero external services required** (works without internet)  \n",
    "> - **Deterministic fallbacks**: you always get runnable code even if a model can’t be downloaded  \n",
    "> - **Portable**: runs on CPU/Windows with no GPU dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4b8f1-3c35-46bd-a0b3-52a85d46a5e2",
   "metadata": {},
   "source": [
    "## Modes at a glance\n",
    "\n",
    "- **`GEN_ONLY`**: One-shot generation. Best for quick demos or sanity checks.  \n",
    "- **`AUTODEBUG`**: Generate → run → if error, try one *greedy* fix. Always falls back to a clean template if still broken.  \n",
    "- **`EVAL_TOY`**: Runs a tiny offline test set with assertions (no downloads). Good for quick regression checks.\n",
    "\n",
    "### Suggested usage\n",
    "- Start with `AUTODEBUG` — it’s forgiving and shows the full loop.\n",
    "- Switch to `EVAL_TOY` to see small but concrete pass/fail signals.\n",
    "- Use `GEN_ONLY` when you just want a snippet fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f45f0d3-b1ef-4149-9efe-4ffffbef0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04dc8536-ed74-4ced-a3d1-6c2df00c285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, textwrap, subprocess, tempfile, ast\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e065451-6d46-42e4-92e1-5d1ed26d216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODE = \"AUTODEBUG\"   # \"GEN_ONLY\" | \"AUTODEBUG\" | \"EVAL_TOY\"\n",
    "MAX_NEW_TOKENS = 180     # (kept modest)\n",
    "TIMEOUT_SEC = 25\n",
    "\n",
    "# tiny → small models; we decode greedily to avoid junk\n",
    "MODEL_CANDIDATES = [\n",
    "    \"sshleifer/tiny-gpt2\",  # ultra tiny demo model (OK for structure)\n",
    "    \"gpt2\",                 # small; slower, but better outputs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ec585d-5c09-4047-8d84-a6c225f80edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_python(code_text: str, timeout: int = TIMEOUT_SEC):\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        fn = os.path.join(td, \"prog.py\")\n",
    "        with open(fn, \"w\", encoding=\"utf-8\") as f: f.write(code_text)\n",
    "        try:\n",
    "            p = subprocess.run([sys.executable, fn], capture_output=True, text=True, timeout=timeout)\n",
    "            return p.returncode, p.stdout, p.stderr, open(fn, encoding=\"utf-8\").read()\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return 124, \"\", f\"[timeout] exceeded {timeout}s\", code_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096dd2a0-de08-4df0-aa96-a7773b581d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_non_ascii(s: str) -> str:\n",
    "    return s.encode(\"ascii\", \"ignore\").decode(\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bcca5ac-c3ee-420b-aafa-4a609c64fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_code(s: str) -> str:\n",
    "    s = re.sub(r\"```(python)?\", \"\", s, flags=re.I).replace(\"```\", \"\")\n",
    "    if \"# Code:\" in s: s = s.split(\"# Code:\", 1)[-1]\n",
    "    s = strip_non_ascii(s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b2ac11-7f4a-465d-98bd-9bd5def64ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_python(code: str) -> bool:\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2844e99-91da-45df-920b-e332b47b57c5",
   "metadata": {},
   "source": [
    "## Design choices\n",
    "\n",
    "### Greedy decoding > sampling (for robustness)\n",
    "We disable sampling (`do_sample=False`) to reduce nonsense tokens from tiny models. This favors **predictability** over creativity.\n",
    "\n",
    "### Sanitization & validation\n",
    "We strip non-ASCII, remove markdown fences, and validate with `ast.parse`. If validation fails, we **fallback** to a known-correct template for the requested task.\n",
    "\n",
    "### Offline-first evaluation\n",
    "The **Toy Dataset** is tiny and local. It checks outcomes with Python `assert` statements, which keeps the evaluation crystal clear.\n",
    "\n",
    "> Trade-off: You won’t get fancy or novel solutions, but you get **reliable, runnable** code every time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e9b590-9b9f-4769-a422-17673c42ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OK, PIPE, LLM_ERR = True, None, None\n",
    "\n",
    "def try_load_llm():\n",
    "    global LLM_OK, PIPE, LLM_ERR\n",
    "    if PIPE is not None or LLM_OK is False: return\n",
    "    try:\n",
    "        for name in MODEL_CANDIDATES:\n",
    "            try:\n",
    "                tok = AutoTokenizer.from_pretrained(name)\n",
    "                if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "                model = AutoModelForCausalLM.from_pretrained(name)\n",
    "                PIPE = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=model, tokenizer=tok,\n",
    "                    # Greedy decoding to avoid nonsense\n",
    "                    do_sample=False,\n",
    "                    temperature=None,\n",
    "                    top_k=None, top_p=None,\n",
    "                    pad_token_id=tok.eos_token_id,\n",
    "                )\n",
    "                print(f\"[llm] loaded {name}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                LLM_ERR = f\"{name}: {e}\"\n",
    "                continue\n",
    "        LLM_OK = False\n",
    "        print(\"[llm] none loaded; using fallback templates.\")\n",
    "        if LLM_ERR: print(\"[llm] last error:\", LLM_ERR)\n",
    "    except Exception as e:\n",
    "        LLM_OK, LLM_ERR = False, str(e)\n",
    "        print(\"[llm] transformers unavailable; using fallback templates.\")\n",
    "        print(\"[llm] error:\", LLM_ERR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6765e38f-5dca-4c4e-a194-b686132d01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(prompt: str, max_new_tokens=MAX_NEW_TOKENS) -> str:\n",
    "    if PIPE is None: raise RuntimeError(\"LLM pipeline not available.\")\n",
    "    out = PIPE(prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "    return sanitize_code(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a5987-abc9-49bb-a31b-e26d9bd09664",
   "metadata": {},
   "source": [
    "## Why templates?\n",
    "\n",
    "Even good small models sometimes emit junk—especially on CPU and with limited context. Templates guarantee a runnable baseline:\n",
    "\n",
    "- If the model output is invalid or unparseable → **template takes over**\n",
    "- Templates are *minimal but correct*; they include a demo in `if __name__ == \"__main__\":`\n",
    "\n",
    "This keeps demos dependable during interviews, workshops, or limited-network environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a43e08-e158-4edc-b9bc-fbe54e25973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback_code_for(task: str) -> str:\n",
    "    t = task.lower()\n",
    "    if \"factorial\" in t:\n",
    "        return \"\"\"def factorial(n):\n",
    "    f = 1\n",
    "    for i in range(2, n+1): f *= i\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(factorial(5))\n",
    "\"\"\"\n",
    "    if \"palindrome\" in t:\n",
    "        return \"\"\"def is_palindrome(s):\n",
    "    s = str(s)\n",
    "    return s == s[::-1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(is_palindrome(\"racecar\"))\n",
    "\"\"\"\n",
    "    if \"reverse\" in t:\n",
    "        return \"\"\"def reverse(s): return s[::-1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(reverse(\"abc\"))\n",
    "\"\"\"\n",
    "    if \"prime\" in t:\n",
    "        return \"\"\"def is_prime(n):\n",
    "    if n < 2: return False\n",
    "    if n % 2 == 0: return n == 2\n",
    "    f = 3\n",
    "    while f*f <= n:\n",
    "        if n % f == 0: return False\n",
    "        f += 2\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(is_prime(29))\n",
    "\"\"\"\n",
    "    return \"\"\"def add(a, b): return a + b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(add(2, 3))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a98e0-ea5c-4fbe-8b00-4d5844c28dc2",
   "metadata": {},
   "source": [
    "## Architecture & flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49a91e-e20e-4d19-8c37-7b4b16fab810",
   "metadata": {},
   "source": [
    "\n",
    "1. **Prompt builder** formats your task into a strict “code-only” instruction.  \n",
    "2. **Generator** tries a tiny local model. If none loads, we use templates.  \n",
    "3. **Sanitizer/validator** ensures we only pass clean, parseable Python to the runner.  \n",
    "4. **Runner** executes in a temp file; we capture `stdout`, `stderr`, and an exit code.  \n",
    "5. **Auto-debug** (optional) applies *one greedy* correction if the run failed and an LLM is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0eca58-efc4-44b7-8022-7206bbfd0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a helpful code generator. Output ONLY valid Python code for a single file, no explanations.\"\n",
    "\n",
    "def build_prompt(task: str) -> str:\n",
    "    return f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "# Task:\n",
    "{task}\n",
    "# Code:\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd73424-bdd8-44ea-b3a4-6c3a2b9754ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fix_prompt(prev_code: str, stderr: str, task: str) -> str:\n",
    "    return textwrap.dedent(f\"\"\"{SYSTEM_PROMPT}\n",
    "The last attempt failed with this error:\n",
    "{stderr}\n",
    "\n",
    "# Task:\n",
    "{task}\n",
    "\n",
    "# Produce corrected code below:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3964e5f2-6294-4938-bbfe-cd9df3986b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_valid_or_fallback(code: str, task: str) -> str:\n",
    "    code = sanitize_code(code)\n",
    "    # Hard checks for typical code structure; enforce validity\n",
    "    if not is_valid_python(code) or (\"def \" not in code and \"if __name__\" not in code):\n",
    "        return fallback_code_for(task)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2860a5ac-1ae6-4e27-9b83-3230a53247ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(task: str) -> str:\n",
    "    try_load_llm()\n",
    "    if LLM_OK:\n",
    "        try:\n",
    "            raw = llm_generate(build_prompt(task))\n",
    "            return ensure_valid_or_fallback(raw, task)\n",
    "        except Exception as e:\n",
    "            print(\"[gen] LLM error; using fallback:\", e)\n",
    "    else:\n",
    "        print(\"[gen] LLM unavailable; using fallback.\")\n",
    "    return fallback_code_for(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c97771b-2a9e-465d-8b5c-246082782437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_debug(task: str, max_attempts: int = 1):\n",
    "    \"\"\"\n",
    "    Generate code, run it, and if it errors AND we have an LLM, try one greedy fix.\n",
    "    If still invalid, return a clean fallback.\n",
    "    \"\"\"\n",
    "    code = generate_code(task)\n",
    "    rc, out, err, cur = run_python(code)\n",
    "    tries = 0\n",
    "    while rc != 0 and tries < max_attempts and LLM_OK:\n",
    "        fixed = llm_generate(build_fix_prompt(cur, err, task))\n",
    "        fixed = ensure_valid_or_fallback(fixed, task)\n",
    "        rc, out, err, cur = run_python(fixed)\n",
    "        code, tries = fixed, tries + 1\n",
    "    if rc != 0:\n",
    "        code = fallback_code_for(task)\n",
    "        rc, out, err, cur = run_python(code)\n",
    "    return rc, out, err, code\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08126b8-7265-4bfe-8ed6-e6f122909917",
   "metadata": {},
   "source": [
    "## Evaluation philosophy\n",
    "\n",
    "- Use **small, unambiguous tasks** that fit in a single file.  \n",
    "- Evaluate with **assertions** (no extra harnesses or packages).  \n",
    "- Favor **binary pass/fail** outcomes you can read at a glance.\n",
    "\n",
    "This approach isn’t meant to “benchmark” LLMs. It’s meant to **prove the loop works** and to help you iterate safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ce7f47-e38a-4ea4-a22f-5a6295c8fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_TASKS = [\n",
    "    {\"prompt\": \"Implement add(a,b) that returns a+b. Include a tiny demo.\", \"tests\": \"assert add(2,3)==5\"},\n",
    "    {\"prompt\": \"Implement reverse(s) that returns reversed string. Demo it.\", \"tests\": \"assert reverse('abc')=='cba'\"},\n",
    "    {\"prompt\": \"Implement factorial(n). Demo in main.\", \"tests\": \"assert factorial(5)==120\"},\n",
    "    {\"prompt\": \"Implement is_prime(n). Demo in main.\", \"tests\": \"assert is_prime(29) is True and is_prime(1) is False\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583be4d8-9c5e-4418-b47b-0d239be98e24",
   "metadata": {},
   "source": [
    "## Extending the toy dataset\n",
    "\n",
    "Want to add your own tasks? Use this pattern:\n",
    "\n",
    "- A short, one-sentence prompt (no ambiguous constraints)\n",
    "- A simple postcondition checked with an `assert`\n",
    "- A tiny demo print for quick visual sanity\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "TOY_TASKS.append({\n",
    "    \"prompt\": \"Implement nth_fib(n) returning the nth Fibonacci (0-indexed). Demo with n=7.\",\n",
    "    \"tests\": \"assert nth_fib(7)==13\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e89d251-6dee-48d4-8e75-129dbc0d5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_toy():\n",
    "    passed = 0\n",
    "    for t in TOY_TASKS:\n",
    "        code = generate_code(t[\"prompt\"])\n",
    "        code += f\"\\n\\n# tests\\n{t['tests']}\\nprint('OK')\\n\"\n",
    "        rc, out, err, _ = run_python(code)\n",
    "        ok = (rc == 0 and \"OK\" in out)\n",
    "        print(f\"[toy] {t['prompt'][:40]}... -> {'PASS' if ok else 'FAIL'}\")\n",
    "        if not ok: print(\"stderr:\", err)\n",
    "        passed += int(ok)\n",
    "    print(f\"[toy] Passed {passed}/{len(TOY_TASKS)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208ca75-b44d-434b-9c44-cff0edb85580",
   "metadata": {},
   "source": [
    "\n",
    "## Reproducibility & determinism\n",
    "\n",
    "- **Greedy decoding** avoids randomness in model outputs.  \n",
    "- **Fallback templates** make success deterministic if the model fails.  \n",
    "- **Assertions** act as exact specs for success.\n",
    "\n",
    "If you later enable sampling for creativity, keep the template fallback as a safety net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b7d37d-2966-47f9-b8f4-6bea279a3228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE: AUTODEBUG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[llm] loaded sshleifer/tiny-gpt2\n",
      "---- RESULT ----\n",
      "return code: 0 \n",
      "stdout:\n",
      " True\n",
      " \n",
      "stderr:\n",
      " \n",
      "----- FINAL CODE -----\n",
      " def is_palindrome(s):\n",
      "    s = str(s)\n",
      "    return s == s[::-1]\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(is_palindrome(\"racecar\"))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode = RUN_MODE.upper().strip()\n",
    "print(\"MODE:\", mode)\n",
    "\n",
    "if mode == \"GEN_ONLY\":\n",
    "    task = \"Write a function factorial(n) that returns n! and include a tiny demo in main.\"\n",
    "    code = generate_code(task)\n",
    "    print(\"----- GENERATED CODE -----\\n\", code)\n",
    "    rc, out, err, _ = run_python(code)\n",
    "    print(\"---- RESULT ----\\nreturn code:\", rc, \"\\nstdout:\\n\", out, \"\\nstderr:\\n\", err)\n",
    "\n",
    "elif mode == \"AUTODEBUG\":\n",
    "    task = \"Write a function to check if a string is a palindrome; print a demo in main.\"\n",
    "    rc, out, err, final_code = auto_debug(task, max_attempts=1)\n",
    "    print(\"---- RESULT ----\\nreturn code:\", rc, \"\\nstdout:\\n\", out, \"\\nstderr:\\n\", err)\n",
    "    print(\"----- FINAL CODE -----\\n\", final_code)\n",
    "\n",
    "elif mode == \"EVAL_TOY\":\n",
    "    eval_toy()\n",
    "\n",
    "else:\n",
    "    print(\"Unknown RUN_MODE. Use: GEN_ONLY | AUTODEBUG | EVAL_TOY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1587af40-42a4-4f66-89b2-697874217ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RC: 0\n",
      "STDOUT:\n",
      " 120\n",
      "\n",
      "FINAL CODE:\n",
      " def factorial(n):\n",
      "    f = 1\n",
      "    for i in range(2, n+1): f *= i\n",
      "    return f\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(factorial(5))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = \"Write a function factorial(n) that returns n! and include a tiny demo in main.\"\n",
    "rc, out, err, final_code = auto_debug(task, max_attempts=1)  # or: code = generate_code(task); rc,out,err,_ = run_python(code)\n",
    "print(\"RC:\", rc); print(\"STDOUT:\\n\", out); print(\"FINAL CODE:\\n\", final_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f684807-8b68-458b-b461-9880109025be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RC: 0\n",
      "STDOUT:\n",
      " True\n",
      "\n",
      "FINAL CODE:\n",
      " def is_prime(n):\n",
      "    if n < 2: return False\n",
      "    if n % 2 == 0: return n == 2\n",
      "    f = 3\n",
      "    while f*f <= n:\n",
      "        if n % f == 0: return False\n",
      "        f += 2\n",
      "    return True\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(is_prime(29))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = \"Write a function is_prime(n) that returns True if n is prime else False; print a quick demo for 29 and 1.\"\n",
    "rc, out, err, final_code = auto_debug(task, max_attempts=1)\n",
    "print(\"RC:\", rc); print(\"STDOUT:\\n\", out); print(\"FINAL CODE:\\n\", final_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d0aa1-189a-4d25-83d5-56402daf1852",
   "metadata": {},
   "source": [
    "## Customization guide\n",
    "\n",
    "### Swap models\n",
    "In the Config cell, reorder `MODEL_CANDIDATES` to prefer `\"gpt2\"` for slightly richer outputs (downloads more). On a GPU, feel free to try a small code model like `\"bigcode/tiny_starcoder\"`.\n",
    "\n",
    "### Add new “skills”\n",
    "Add a new branch in `fallback_code_for(task)` for a task pattern you care about, e.g., CSV parsing, simple math utilities, or string formatting.\n",
    "\n",
    "### Hardening\n",
    "- Increase `TIMEOUT_SEC` for heavier tasks\n",
    "- Add stricter validators (e.g., require specific function signatures)\n",
    "- Log runs to disk (timestamped files) for auditing\n",
    "\n",
    "### Turning this into a CLI\n",
    "You can wrap the `generate_code`, `auto_debug`, and `eval_toy` functions with `argparse` to build a tiny command-line tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda125fe-be32-4968-bee4-0830abf5c5fb",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "**Q: Why do I sometimes see the fallback code even when an LLM loaded?**  \n",
    "A: We validate outputs with `ast.parse`. If the model produces invalid Python, the fallback guarantees a working snippet. This is by design.\n",
    "\n",
    "**Q: Can I make the model more creative?**  \n",
    "A: Yes—switch to `\"gpt2\"` as the first candidate and enable sampling in the loader. Keep the validator and fallback in place to avoid breaking runs.\n",
    "\n",
    "**Q: How do I run bigger tasks?**  \n",
    "A: Increase `TIMEOUT_SEC`, raise `MAX_NEW_TOKENS`, and consider a stronger local model or a hosted API. For complex tasks, switch to multi-file with tests, but keep the same run/validate pattern.\n",
    "\n",
    "**Q: Why not use MBPP/HumanEval directly here?**  \n",
    "A: This notebook is *offline-first*. MBPP/HumanEval requires downloads and a test harness. For a heavier project, you can integrate them in a separate section while keeping this simple loop as a quick sanity stage.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d773b52-66fc-4b85-89ae-73517389e00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
